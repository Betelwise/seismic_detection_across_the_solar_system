{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (73, 5565), Labels shape: (73,), Auxiliary data shape: (73, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from obspy import read\n",
    "from obspy.signal.filter import bandpass\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "#csv_file = './data/lunar/training/catalogs/initial_filtered_events_of_S12_GradeB.csv'\n",
    "#csv_file = './data/lunar/training/catalogs/initial_filtered_events_of_Mars_data.csv'\n",
    "csv_file = './data/lunar/training/catalogs/initial_filtered_events_of_Mars_data_2.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Create empty lists to store the data and labels\n",
    "data_list = []\n",
    "labels = []\n",
    "aux_data_list = []  # To store auxiliary data (std dev before and after)\n",
    "\n",
    "# Define the fixed length for the data (e.g., 14 minutes of data at a specific sampling rate)\n",
    "fixed_length = 5565  # Example: 14 minutes of data at 100 Hz sampling rate\n",
    "\n",
    "# Loop through each row in the CSV file\n",
    "for idx, row in df.iterrows():\n",
    "    filename = row['filename']\n",
    "    time_rel = row['time_rel(sec)']  # Relative time point in seconds\n",
    "    filter_low = row['Filter_low_freq_point']  # Low cut-off frequency\n",
    "    filter_high = row['Filter_high_freq_point']  # High cut-off frequency\n",
    "    label = row['label']  # Event label (0 or 1)\n",
    "\n",
    "    # Load the MiniSEED file\n",
    "    #data_directory = './data/lunar/test/data/S12_GradeB/'\n",
    "    data_directory = './data/mars/test/data/'\n",
    "    mseed_file = f'{data_directory}{filename}.mseed'\n",
    "    st = read(mseed_file)\n",
    "\n",
    "    # Apply bandpass filter (between filter_low and filter_high)\n",
    "    filtered_st = st.filter('bandpass', freqmin=filter_low, freqmax=filter_high)\n",
    "\n",
    "    # Define the start and end times for slicing the data (4 minutes before and 10 minutes after time_rel)\n",
    "    start_time = time_rel - 4 * 60  # 4 minutes before\n",
    "    end_time = time_rel + 10 * 60  # 10 minutes after\n",
    "\n",
    "    # Cut the data from start_time to end_time\n",
    "    sliced_st = filtered_st.slice(starttime=st[0].stats.starttime + start_time, \n",
    "                                  endtime=st[0].stats.starttime + end_time)\n",
    "\n",
    "    # Normalize the sliced data (mean=0, std=1)\n",
    "    sliced_data = sliced_st[0].data\n",
    "    sliced_data_normalized = (sliced_data - np.mean(sliced_data)) / np.std(sliced_data)\n",
    "\n",
    "    # Calculate standard deviation before and after time_rel\n",
    "    before_data = filtered_st.slice(starttime=st[0].stats.starttime + start_time, \n",
    "                                     endtime=st[0].stats.starttime + time_rel)\n",
    "    after_data = filtered_st.slice(starttime=st[0].stats.starttime + time_rel, \n",
    "                                    endtime=st[0].stats.starttime + end_time)\n",
    "    \n",
    "    std_before = np.std(before_data[0].data)\n",
    "    std_after = np.std(after_data[0].data)\n",
    "\n",
    "    # Pad or truncate the data to the fixed length\n",
    "    if len(sliced_data_normalized) > fixed_length:\n",
    "        sliced_data_normalized = sliced_data_normalized[:fixed_length]\n",
    "    else:\n",
    "        sliced_data_normalized = np.pad(sliced_data_normalized, (0, fixed_length - len(sliced_data_normalized)), 'constant')\n",
    "\n",
    "    # Store the normalized data, labels, and auxiliary data\n",
    "    data_list.append(sliced_data_normalized)\n",
    "    labels.append(label)\n",
    "    aux_data_list.append([std_before, std_after])  # Append standard deviations\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X_data = np.array(data_list)\n",
    "y_labels = np.array(labels)\n",
    "aux_data = np.array(aux_data_list)  # Auxiliary data containing std devs\n",
    "\n",
    "# Now, X_data contains the processed seismogram data,\n",
    "# y_labels contains the labels, and aux_data contains the std devs\n",
    "print(f\"Data shape: {X_data.shape}, Labels shape: {y_labels.shape}, Auxiliary data shape: {aux_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23.3319032  39.60915098]\n",
      " [49.16240813 32.35977052]\n",
      " [45.88701926 32.33288439]\n",
      " [27.11691199 32.50738732]\n",
      " [20.46323321 32.6581128 ]\n",
      " [27.00562632 30.21179258]\n",
      " [40.99128116 22.25301133]\n",
      " [19.11677057 20.56718148]\n",
      " [19.14967878 20.64673211]\n",
      " [20.75694491 17.85269018]]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(aux_data[:10])\n",
    "print(y_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assume X_data and y_labels are your arrays\n",
    "np.save('X_data3.npy', X_data)\n",
    "np.save('y_labels3.npy', y_labels)\n",
    "np.save('aux_data3.npy', aux_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
