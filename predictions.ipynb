{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from obspy import read\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from ipywidgets import Button, VBox, HBox, Output, Label\n",
    "from IPython.display import display\n",
    "import glob\n",
    "from scipy import signal\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predicitons of Seismic Data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Level 1 of Filtering the data\n",
    "\n",
    "### Adjusting settings to short list potential events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define settings for Moon and Mars\n",
    "settings = {\n",
    "    'moon': {\n",
    "        'low_freq_point': 0.2,#0.6\n",
    "        'high_freq_point': 1,\n",
    "        'frequence_window': 0.2,\n",
    "        'sta_len': 100,\n",
    "        'lta_len': 1000,\n",
    "        'thr_on': 3,\n",
    "        'resample': False\n",
    "    },\n",
    "    'mars': {\n",
    "        'low_freq_point': 0.6,\n",
    "        'high_freq_point': 4,\n",
    "        'frequence_window': 0.5,\n",
    "        'sta_len': 20,\n",
    "        'lta_len': 80,\n",
    "        'thr_on': 2.1,\n",
    "        'resample': True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to get settings based on event type\n",
    "def get_settings(event_type):\n",
    "    return settings.get(event_type, settings['moon'])  # Default to 'moon' if event_type is not found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the Files to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mseed_files = sorted(glob.glob('./data/mars/training/data/*.mseed'))\n",
    "#mseed_files = sorted(glob.glob('./data/mars/test/data/*.mseed'))\n",
    "\n",
    "mseed_files = sorted(glob.glob('./data/lunar/training/data/S12_GradeA/*.mseed'))\n",
    "#mseed_files = sorted(glob.glob('./data/lunar/test/data/S12_GradeB/*.mseed'))\n",
    "#mseed_files = sorted(glob.glob('./data/lunar/test/data/S15_GradeA/*.mseed'))\n",
    "#mseed_files = sorted(glob.glob('./data/lunar/test/data/S15_GradeB/*.mseed'))\n",
    "#mseed_files = sorted(glob.glob('./data/lunar/test/data/S16_GradeA/*.mseed'))\n",
    "#mseed_files = sorted(glob.glob('./data/lunar/test/data/S16_GradeB/*.mseed'))\n",
    "\n",
    "event_info = []\n",
    "filename = ''\n",
    "\n",
    "\n",
    "# Determine event type based on the file path\n",
    "if 'lunar' in mseed_files[0]:\n",
    "    event_type = 'moon'\n",
    "elif 'mars' in mseed_files[0]:\n",
    "    event_type = 'mars'\n",
    "else:\n",
    "    event_type = 'moon'  # Default to 'moon' if neither is found\n",
    "\n",
    "# Get settings based on the determined event type\n",
    "current_settings = get_settings(event_type)\n",
    "plot = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essential Functions definations\n",
    "#### calculating best frequency range to filter data with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mseed(file_idx):\n",
    "    st = read(mseed_files[file_idx])\n",
    "    return st, mseed_files[file_idx]\n",
    "\n",
    "\n",
    "\n",
    "def calculate_best_freq_range_by_std(st, resample):\n",
    "    # Define frequency ranges for filtering\n",
    "    freq_ranges = [\n",
    "          (start, start + current_settings['frequence_window']) \n",
    "          for start in np.arange(\n",
    "              current_settings['low_freq_point'], \n",
    "              current_settings['high_freq_point'], \n",
    "              0.1)]\n",
    "    \n",
    "    stddev_in_ranges = []\n",
    "    for f_min, f_max in freq_ranges:\n",
    "        # Copy the original trace for filtering\n",
    "        st_filt = st.copy()\n",
    "        # Apply bandpass filter for the given frequency range\n",
    "        st_filt.filter('bandpass', freqmin=f_min, freqmax=f_max)\n",
    "        # Extract the filtered trace data\n",
    "        tr_filt = st_filt.traces[0].copy()\n",
    "        #if(resample):\n",
    "            #tr_filt.resample(6.625)\n",
    "        tr_data_filt = tr_filt.data\n",
    "        # Normalize tr_data_filt between -1 and 1\n",
    "        tr_data_filt = 2 * (tr_data_filt - np.min(tr_data_filt)) / (np.max(tr_data_filt) - np.min(tr_data_filt)) - 1\n",
    "        \n",
    "        # Calculate standard deviation of the filtered signal\n",
    "        stddev = np.std(tr_data_filt)\n",
    "        stddev_in_ranges.append((f_min, f_max, stddev))\n",
    "    # Find the frequency range with the minimum standard deviation\n",
    "    best_range = min(stddev_in_ranges, key=lambda x: x[2])\n",
    "\n",
    "    print(f\"The best frequency range is {best_range[0]:.1f} to {best_range[1]:.1f} with minimum standard deviation {best_range[2]:.3f}\")\n",
    "    return best_range \n",
    "\n",
    "def calculate_best_freq_range_by_pwr(st, resample):\n",
    "    # Extract the trace data and sampling rate\n",
    "    tr = st.traces[0].copy()\n",
    "    sampling_rate = tr.stats.sampling_rate\n",
    "    tr_data = tr.data\n",
    "\n",
    "    # Define frequency ranges for filtering\n",
    "    freq_ranges = [\n",
    "        (start, start + current_settings['frequence_window']) \n",
    "        for start in np.arange(\n",
    "            current_settings['low_freq_point'], \n",
    "            current_settings['high_freq_point'],\n",
    "            0.1\n",
    "        )\n",
    "    ]\n",
    "    # Initialize list to store top 1000 power averages for each frequency range\n",
    "    max_power_in_ranges = []\n",
    "\n",
    "    # Loop through each frequency range\n",
    "    for f_min, f_max in freq_ranges:\n",
    "        # Copy the original trace for filtering\n",
    "        st_filt = st.copy()\n",
    "\n",
    "        # Apply bandpass filter for the given frequency range\n",
    "        st_filt.filter('bandpass', freqmin=f_min, freqmax=f_max)\n",
    "\n",
    "        # Extract the filtered trace data\n",
    "        tr_filt = st_filt.traces[0].data\n",
    "\n",
    "        # Compute the spectrogram for the filtered signal\n",
    "        fu, tu, sxxu = signal.spectrogram(tr_filt, fs=sampling_rate)\n",
    "        sxxu_dB = 10 * np.log10(sxxu)\n",
    "\n",
    "        # Flatten the entire spectrogram (all frequencies and time points)\n",
    "        power_values_flat = sxxu_dB.flatten()\n",
    "\n",
    "        # Sort the power values and take the top 1000\n",
    "        top_1000_power_values = np.sort(power_values_flat)[-50:]  # Top 1000 largest values\n",
    "\n",
    "        # Calculate the average of the top 1000 values\n",
    "        top_1000_avg_power = np.mean(top_1000_power_values)\n",
    "        print(top_1000_avg_power)\n",
    "\n",
    "        # Store the frequency range and the calculated average power\n",
    "        max_power_in_ranges.append((f_min, f_max, top_1000_avg_power))\n",
    "\n",
    "    # Find the frequency range with the greatest average of the top 1000 power values\n",
    "    best_range = max(max_power_in_ranges, key=lambda x: x[2])\n",
    "\n",
    "    print(f\"The best frequency range is {best_range[0]:.1f} to {best_range[1]:.1f} with average power {best_range[2]:.3f}\")\n",
    "    return best_range\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Events according to 1st Filter Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_file_idx = 0    \n",
    "def detect_events(max_files = 100):\n",
    "    num_files_to_display = len(mseed_files)\n",
    "    if(num_files_to_display > max_files):\n",
    "        num_files_to_display = max_files\n",
    "\n",
    "    print('Detecting events.....')\n",
    "    print('Event Type is :', event_type)\n",
    "    if(plot):\n",
    "        # Create a vertical plot layout based on the number of files (each file gets 4 plots)\n",
    "        fig, axs = plt.subplots(num_files_to_display, 2, figsize=(20, 5 * num_files_to_display))\n",
    "    resample = current_settings['resample']\n",
    "    event_number = 0\n",
    "    for i in range(num_files_to_display):\n",
    "        global filename\n",
    "        # Load the MiniSEED file\n",
    "        st, filename = load_mseed(current_file_idx + i)  # Incrementing file index\n",
    "        # Extract the evid part from the filename\n",
    "        evid = filename.split('_')[-1].split('.')[0]\n",
    "        filename = filename\n",
    "        \n",
    "        \n",
    "        #best_range = calculate_best_freq_range_by_std(st, resample)\n",
    "        best_range = calculate_best_freq_range_by_pwr(st, resample)\n",
    "        # Define the frequency range for the bandpass filter\n",
    "        # minfreq = 0.8\n",
    "        # maxfreq = 0.9\n",
    "        minfreq = best_range[0]\n",
    "        maxfreq = best_range[1]\n",
    "\n",
    "        # ============================Apply the filter\n",
    "        st_filt = st.copy()\n",
    "        st_filt.filter('bandpass', freqmin=minfreq, freqmax=maxfreq)\n",
    "        #st_filt.filter('bandpass', freqmin=3, freqmax=4)\n",
    "        if(resample):\n",
    "            st_filt.resample(6.625)\n",
    "        tr_filt = st_filt.traces[0].copy()\n",
    "        tr_times_filt = tr_filt.times()\n",
    "        tr_data_filt = tr_filt.data\n",
    "        stddev = np.std(tr_data_filt)\n",
    "        threshold_upper = 26 * stddev\n",
    "        threshold_lower = -26 * stddev\n",
    "        tr_data_filt[np.abs(tr_data_filt) > threshold_upper] = stddev \n",
    "        tr_data_filt[np.abs(tr_data_filt) < threshold_lower] = -stddev\n",
    "        # Normalize tr_data_filt between -1 and 1\n",
    "        tr_data_filt = 2 * (tr_data_filt - np.min(tr_data_filt)) / (np.max(tr_data_filt) - np.min(tr_data_filt)) - 1\n",
    "\n",
    "        from obspy.signal.trigger import classic_sta_lta, plot_trigger, trigger_onset\n",
    "\n",
    "        sta_len = current_settings['sta_len']\n",
    "        lta_len = current_settings['lta_len']\n",
    "\n",
    "        tr_data_abs = np.abs(tr_data_filt)\n",
    "        df = 6.625\n",
    "        # Run Obspy's STA/LTA to obtain a characteristic function\n",
    "        cft = classic_sta_lta(tr_data_abs, int(sta_len * df), int(lta_len * df))\n",
    "        average_cft = np.mean(cft)\n",
    "        print(f'Average CFT: {average_cft}')\n",
    "\n",
    "        thr_on = current_settings['thr_on']\n",
    "        thr_off = average_cft\n",
    "        on_off = np.array(trigger_onset(cft, thr_on, thr_off))\n",
    "\n",
    "\n",
    "        cat_directory = './data/lunar/training/catalogs/'\n",
    "        cat_file = cat_directory + 'apollo12_catalog_GradeA_final.csv'\n",
    "        cat = pd.read_csv(cat_file)\n",
    "        row = cat.iloc[i]\n",
    "        arrival_time_rel = row['time_rel(sec)']\n",
    "\n",
    "        if(plot):\n",
    "            # Plot filtered trace in the first column\n",
    "            axs[i, 0].plot(tr_times_filt, tr_data_filt)\n",
    "            axs[i, 0].set_xlim([min(tr_times_filt), max(tr_times_filt)])\n",
    "            axs[i, 0].set_ylabel('Velocity (m/s)')\n",
    "            axs[i, 0].set_xlabel('Time (s)')\n",
    "            axs[i, 0].set_title(f'Filtered Time Series of {evid} with {i}')\n",
    "            arrival_line = axs[i, 0].axvline(x=arrival_time_rel, c='purple', label='Abs. Arrival')\n",
    "            axs[i, 0].annotate(f'Arrival TIme at {arrival_time_rel}', xy=(arrival_time_rel, tr_data_filt.max()), \n",
    "                            xytext=(arrival_time_rel + 0.1, tr_data_filt.max()+0.1),  # Adjust text position\n",
    "                            color='black', fontsize=11)\n",
    "            for j in np.arange(0, len(on_off)):\n",
    "                triggers = on_off[j]\n",
    "                if(triggers[1] - triggers[0] > 1200) or (len(on_off) < 50):\n",
    "                    event_number += 1\n",
    "                    exact_filename = filename.split('\\\\')[-1].rsplit('.', 1)[0]\n",
    "                    event_info.append([exact_filename, event_number, f'{i} | {j}', (round(tr_times_filt[triggers[0]]-100)), 0, round(best_range[0],1), round(best_range[1],1),  ])\n",
    "                    # Draw the vertical line\n",
    "                    axs[i, 0].axvline(x=tr_times_filt[triggers[0]], color='red', label=f'Trig. On {j+1}')\n",
    "                    #print(f'Trig. On {i+1} of {j+1} at {triggers[0]}')\n",
    "                    # Annotate with the trigger number (or any custom label)\n",
    "                    dj = (j - 1) % 16 + 1\n",
    "                    axs[i, 0].annotate(f'Trig {j} at {round(tr_times_filt[triggers[0]])}s', xy=(tr_times_filt[triggers[0]], tr_data_filt.max()), \n",
    "                                    xytext=(tr_times_filt[triggers[0]] + 0.1, tr_data_filt.max()-(dj/8)),  # Adjust text position\n",
    "                                    color='grey', fontsize=11)\n",
    "            # Plot characteristic function\n",
    "            axs[i,1].plot(tr_times_filt,cft)\n",
    "            axs[i,1].set_xlim([min(tr_times_filt),max(tr_times_filt)])\n",
    "            axs[i,1].set_xlabel('Time (s)')\n",
    "            axs[i,1].set_ylabel('Characteristic function')\n",
    "\n",
    "        \n",
    "    if(plot):\n",
    "        # Adjust layout for better spacing\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initializing event detection for 1st level filteration\n",
    "#### select how many to filter in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detect_events(max_files = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving 1st filter results in catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = filename.split('/')[-1].split('\\\\')[0]\n",
    "# Convert the event information list to a DataFrame\n",
    "event_df = pd.DataFrame(event_info, columns=['filename', 'Event Number','events_in_day(day|event)', 'time_rel(sec)', 'time_abs(sec)', 'Filter_low_freq_point', 'Filter_high_freq_point'])\n",
    "# Define the folder path\n",
    "folder_path = f'initial_filterations_catalogue/{event_type}'\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "savePath = f'{folder_path}/initial_filtered_events_of_{folder_name}.csv'\n",
    "event_df.to_csv(savePath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Catalogue files and making numpy arrays according to them\n",
    "#### for CNN to make final Prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.signal.filter import bandpass\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = savePath\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Create empty lists to store the data and labels\n",
    "data_list = []\n",
    "labels = []\n",
    "aux_data_list = []  # To store auxiliary data (std dev before and after)\n",
    "\n",
    "# Define the fixed length for the data (e.g., 14 minutes of data at a specific sampling rate)\n",
    "fixed_length = 5565  # Example: 14 minutes of data at 100 Hz sampling rate\n",
    "\n",
    "# Loop through each row in the CSV file\n",
    "for idx, row in df.iterrows():\n",
    "    filename = row['filename']\n",
    "    time_rel = row['time_rel(sec)']  # Relative time point in seconds\n",
    "    filter_low = row['Filter_low_freq_point']  # Low cut-off frequency\n",
    "    filter_high = row['Filter_high_freq_point']  # High cut-off frequency\n",
    "    label = row['label']  # Event label (0 or 1)\n",
    "\n",
    "    # Load the MiniSEED file\n",
    "    data_directory = './data/lunar/training/data/S12_GradeA/'\n",
    "    mseed_file = f'{data_directory}{filename}.mseed'\n",
    "    st = read(mseed_file)\n",
    "\n",
    "    # Apply bandpass filter (between filter_low and filter_high)\n",
    "    filtered_st = st.filter('bandpass', freqmin=filter_low, freqmax=filter_high)\n",
    "\n",
    "    # Define the start and end times for slicing the data (4 minutes before and 10 minutes after time_rel)\n",
    "    start_time = time_rel - 4 * 60  # 4 minutes before\n",
    "    end_time = time_rel + 10 * 60  # 10 minutes after\n",
    "\n",
    "    # Cut the data from start_time to end_time\n",
    "    sliced_st = filtered_st.slice(starttime=st[0].stats.starttime + start_time, \n",
    "                                  endtime=st[0].stats.starttime + end_time)\n",
    "\n",
    "    # Normalize the sliced data (mean=0, std=1)\n",
    "    sliced_data = sliced_st[0].data\n",
    "    sliced_data_normalized = (sliced_data - np.mean(sliced_data)) / np.std(sliced_data)\n",
    "\n",
    "    # Calculate standard deviation before and after time_rel\n",
    "    before_data = filtered_st.slice(starttime=st[0].stats.starttime + start_time, \n",
    "                                     endtime=st[0].stats.starttime + time_rel)\n",
    "    after_data = filtered_st.slice(starttime=st[0].stats.starttime + time_rel, \n",
    "                                    endtime=st[0].stats.starttime + end_time)\n",
    "    \n",
    "    std_before = np.std(before_data[0].data)\n",
    "    std_after = np.std(after_data[0].data)\n",
    "\n",
    "    # Pad or truncate the data to the fixed length\n",
    "    if len(sliced_data_normalized) > fixed_length:\n",
    "        sliced_data_normalized = sliced_data_normalized[:fixed_length]\n",
    "    else:\n",
    "        sliced_data_normalized = np.pad(sliced_data_normalized, (0, fixed_length - len(sliced_data_normalized)), 'constant')\n",
    "\n",
    "    # Store the normalized data, labels, and auxiliary data\n",
    "    data_list.append(sliced_data_normalized)\n",
    "    labels.append(label)\n",
    "    aux_data_list.append([std_before, std_after])  # Append standard deviations\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X_data = np.array(data_list)\n",
    "y_labels = np.array(labels)\n",
    "aux_data = np.array(aux_data_list)  # Auxiliary data containing std devs\n",
    "\n",
    "# Now, X_data contains the processed seismogram data,\n",
    "# y_labels contains the labels, and aux_data contains the std devs\n",
    "print(f\"Data shape: {X_data.shape}, Labels shape: {y_labels.shape}, Auxiliary data shape: {aux_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
